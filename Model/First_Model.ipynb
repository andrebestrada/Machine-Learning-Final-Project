{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c9e9f8e",
   "metadata": {},
   "source": [
    "# Modelo 01\n",
    "\n",
    "<hr>\n",
    "## Cleaning and Processing Machine Learning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d2b55884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "390d114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# houses = json.load(open('../../EDOMEX_houses.json'))\n",
    "# apartments = json.load(open('../../EDOMEX_apartments.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1498de8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_houses = pd.read_json('../../EDOMEX_houses.json')\n",
    "df_apartments = pd.read_json('../../EDOMEX_apartments.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b35f9c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete = df_houses.append(df_apartments)\n",
    "# df_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4a8b8b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete = df_complete[df_complete['currency']==\"MXN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d577001",
   "metadata": {},
   "source": [
    "## Split data Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "68671928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read data\n",
    "X_full = df_complete\n",
    "# X_test_full = pd.read_csv(\"\")\n",
    "\n",
    "# Obtain Target and Predictions\n",
    "y = X_full.price\n",
    "\n",
    "features = ['type_of_prop',\n",
    "       'Estado', 'Ciudad', 'Colonia', 'Superficie total',\n",
    "       'Superficie construida', 'Ambientes', 'Recamaras', 'Banos',\n",
    "       'Estacionamientos', 'Antiguedad', 'Cantidad de pisos',\n",
    "       'Cuota mensual de mantenimiento', 'Bodegas']\n",
    "\n",
    "X = X_full[features]\n",
    "# X_test = X_test_full[features]\n",
    "\n",
    "# Break off validation set from training data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e8a5673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define the models\n",
    "model_1 = RandomForestRegressor(n_estimators=50, random_state=0)\n",
    "model_2 = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "model_3 = RandomForestRegressor(n_estimators=100, criterion='mae', random_state=0)\n",
    "model_4 = RandomForestRegressor(n_estimators=200, min_samples_split=20, random_state=0)\n",
    "model_5 = RandomForestRegressor(n_estimators=100, max_depth=7, random_state=0)\n",
    "\n",
    "models = [model_1, model_2, model_3, model_4, model_5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c7aa14",
   "metadata": {},
   "source": [
    "## 1) Cleaning\n",
    "\n",
    "We require the transformation of Categorical Variables to feed the model, we will use One Hot Encoder because the variables we need to transform do not have an ordinal rank, and we donÂ´t need to identify a hierarchy level between houses and apartments, for example. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3866098",
   "metadata": {},
   "source": [
    "### Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b7c3de30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical variables:\n",
      "['type_of_prop', 'Estado', 'Ciudad', 'Colonia']\n"
     ]
    }
   ],
   "source": [
    "# Get list of categorical variables\n",
    "s = (X_train.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "print(\"Categorical variables:\")\n",
    "print(object_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3069f4a2",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "75320e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dabd437",
   "metadata": {},
   "source": [
    "### Goodbye NAs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "519f6bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more NAs, now just 0s :D\n"
     ]
    }
   ],
   "source": [
    "OH_X_train=OH_X_train.fillna(0)\n",
    "OH_X_valid=OH_X_valid.fillna(0)\n",
    "\n",
    "print(\"No more NAs, now just 0s :D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7614fed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 MAE: 1331317\n",
      "Model 2 MAE: 1315987\n",
      "Model 3 MAE: 1277496\n",
      "Model 4 MAE: 1417299\n",
      "Model 5 MAE: 1348177\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Function for comparing different models\n",
    "def score_model(model, X_t=OH_X_train, X_v=OH_X_valid, y_t=y_train, y_v=y_valid):\n",
    "    model.fit(X_t, y_t)\n",
    "    preds = model.predict(X_v)\n",
    "    return mean_absolute_error(y_v, preds)\n",
    "\n",
    "for i in range(0, len(models)):\n",
    "    mae = score_model(models[i])\n",
    "    print(\"Model %d MAE: %d\" % (i+1, mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dba9f2a",
   "metadata": {},
   "source": [
    "## 2) Cross Validation\n",
    "\n",
    "Cross Validation is used to improve the score measures of our model by running the same process on different subsets of data to get multiple scores, or in other workds, desing different scenarios in order to use the 100% of the dataset as a test or validation data. This method should be used if we have a relative small data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8750ef31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Pipeline\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "my_pipeline = Pipeline(steps=[('model', RandomForestRegressor(n_estimators=50,\n",
    "                                                              random_state=0))\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ddabba4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Avg Cross Score: 1628782\n",
      "Model 2 Avg Cross Score: 1639069\n",
      "Model 3 Avg Cross Score: 1640996\n",
      "Model 4 Avg Cross Score: 1611346\n",
      "Model 5 Avg Cross Score: 1644486\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Get list of categorical variables\n",
    "s = (X.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "# Encoding Categorical Variables with OneHotEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_X_pandas = pd.get_dummies(X)\n",
    "\n",
    "# OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "# OH_cols = pd.DataFrame(OH_encoder.fit_transform(X[object_cols]))\n",
    "# One-hot encoding removed index; put it back\n",
    "# OH_cols.index = X.index\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "# num_X = X.drop(object_cols, axis=1)\n",
    "# Add one-hot encoded columns to numerical features\n",
    "# OH_X = pd.concat([num_X, OH_cols], axis=1)\n",
    "\n",
    "# Verify if NA cleaning can be inside the pipeline\n",
    "# OH_X=OH_X.fillna(0)\n",
    "\n",
    "def crossed_score_model(model, X, y):\n",
    "    \n",
    "    my_pipeline = Pipeline(steps=[('model', model)])\n",
    "    \n",
    "    # Multiply by -1 since sklearn calculates *negative* MAE\n",
    "    scores = -1 * cross_val_score(my_pipeline, X, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "#     print(\"MAE scores:\\n\", scores)\n",
    "#     print(scores.mean())\n",
    "    return scores.mean()\n",
    "\n",
    "for i in range(0,len(models)):\n",
    "    cross_score=crossed_score_model(models[i],OH_X_pandas, y)\n",
    "    print(\"Model %d Avg Cross Score: %d\" % (i+1, cross_score))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6f0542",
   "metadata": {},
   "source": [
    "## 3) XG Boost\n",
    "\n",
    "Gradient boosting is a method that goes through cycles to iteratively add models into an ensemble.\n",
    "\n",
    "It begins by initializing the ensemble with a single model, whose predictions can be pretty naive. (Even if its predictions are wildly inaccurate, subsequent additions to the ensemble will address those errors.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "480d3487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1473643.119047619\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "X_valid \n",
    "\n",
    "xg_model = XGBRegressor()\n",
    "xg_model.fit(OH_X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Get predictions\n",
    "predictions = xg_model.predict(OH_X_valid) \n",
    "mae = mean_absolute_error(predictions, y_valid) \n",
    "\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "eae2897c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE still very high :(\n"
     ]
    }
   ],
   "source": [
    "print(\"MAE still very high :(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f891a45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
