{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Network with Keras"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import os \r\n",
    "import json\r\n",
    "import json"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Set the seed value for the notebook so the results are reproducible\r\n",
    "# from numpy.random import seed\r\n",
    "# seed(42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Generate Data\r\n",
    "national_data = pd.read_json(\"../../Scraping/Output/full_data.json\")\r\n",
    "national_data.columns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "national_data = national_data[national_data['currency']=='MXN']\r\n",
    "national_data = national_data[national_data['name'].str.contains('Renta')==False]\r\n",
    "national_data = national_data[national_data['cp'].isnull()==False]\r\n",
    "\r\n",
    "national_data = national_data[national_data['price']>1000000]\r\n",
    "national_data = national_data[national_data['price']<10000000]\r\n",
    "#national_data = national_data[national_data['Estado']=='Puebla']\r\n",
    "national_data = national_data[national_data['Superficie total']<300]\r\n",
    "\r\n",
    "\r\n",
    "# national_data['price'].plot.box(grid='True')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Replace NA's with 0\r\n",
    "\r\n",
    "national_data_clean = national_data.fillna(0)\r\n",
    "# national_data_clean.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cleaning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "#Â Read Data\r\n",
    "\r\n",
    "X_full = national_data_clean\r\n",
    "# X_test_full = pd.read_csv(\"\")\r\n",
    "\r\n",
    "# Obtain Target and Predictions\r\n",
    "y = X_full.price\r\n",
    "\r\n",
    "features = ['type_of_prop', 'cp', 'Superficie total',\r\n",
    "       'Superficie construida', 'Ambientes', 'Recamaras', 'Banos',\r\n",
    "       'Estacionamientos', 'Antiguedad', 'Cantidad de pisos',\r\n",
    "       'Cuota mensual de mantenimiento', 'Bodegas']\r\n",
    "\r\n",
    "X = X_full[features]\r\n",
    "\r\n",
    "\r\n",
    "# Break off validation set from training data\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get list of categorical variables\r\n",
    "s = (X_train.dtypes == 'object')\r\n",
    "object_cols = list(s[s].index)\r\n",
    "\r\n",
    "print(\"Categorical variables:\")\r\n",
    "print(object_cols)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# One-hot encoding\r\n",
    "y_train_categorical = to_categorical(y_train)\r\n",
    "y_test_categorical = to_categorical(y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# first, create a normal neural network with 2 inputs, 6 hidden nodes, and 2 outputs\r\n",
    "# Defining Our Model Architecture\r\n",
    "\r\n",
    "from tensorflow.keras.models import Sequential\r\n",
    "from tensorflow.keras.layers import Dense\r\n",
    "\r\n",
    "model = Sequential()\r\n",
    "model.add(Dense(units=6, activation='relu', input_dim=2))\r\n",
    "model.add(Dense(units=2, activation='softmax'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Compile the model\r\n",
    "model.compile(optimizer='adam',\r\n",
    "              loss='categorical_crossentropy',\r\n",
    "              metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Fit (train) the model\r\n",
    "model.fit(\r\n",
    "    X_train_scaled,\r\n",
    "    y_train_categorical,\r\n",
    "    epochs=10000,\r\n",
    "    shuffle=True,\r\n",
    "    verbose=2\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# One Hot Encoding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\"\"\r\n",
    "from sklearn.preprocessing import OneHotEncoder\r\n",
    "\r\n",
    "s = (X_train.dtypes == 'object')\r\n",
    "object_cols = list(s[s].index)\r\n",
    "\r\n",
    "# Apply one-hot encoder to each column with categorical data\r\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\r\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\r\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\r\n",
    "\r\n",
    "# One-hot encoding removed index; put it back\r\n",
    "OH_cols_train.index = X_train.index\r\n",
    "OH_cols_valid.index = X_valid.index\r\n",
    "\r\n",
    "# Remove categorical columns (will replace with one-hot encoding)\r\n",
    "num_X_train = X_train.drop(object_cols, axis=1)\r\n",
    "num_X_valid = X_valid.drop(object_cols, axis=1)\r\n",
    "\r\n",
    "# Add one-hot encoded columns to numerical features\r\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\r\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\r\n",
    "\"\"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Transform the training and testing data using the X_scaler\r\n",
    "\r\n",
    "X_train_scaled = X_scaler.transform(X_train)\r\n",
    "X_test_scaled = X_scaler.transform(X_test)\r\n",
    "\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "\r\n",
    "# Create a StandardScater model and fit it to the training data\r\n",
    "X_scaler = StandardScaler().fit(X_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create a StandardScater model and fit it to the training data\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "X_scaler = StandardScaler().fit(OH_X_train)\r\n",
    "\r\n",
    "# Transform and Training\r\n",
    "X_train_scaled = X_scaler.transform(OH_X_train)\r\n",
    "X_test_scaled = X_scaler.transform(OH_X_valid)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# first, create a normal neural network with 2 inputs, 6 hidden nodes, and 2 outputs\r\n",
    "# Defining Our Model Architecture\r\n",
    "\r\n",
    "from tensorflow.keras.models import Sequential\r\n",
    "from tensorflow.keras.layers import Dense\r\n",
    "\r\n",
    "model = Sequential()\r\n",
    "model.add(Dense(units=6, activation='relu', input_dim=12))\r\n",
    "model.add(Dense(units=1, activation='softmax'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Compile the Model\r\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Fit (train) the model\r\n",
    "model.fit(\r\n",
    "    X_train_scaled,\r\n",
    "    y_train_categorical,\r\n",
    "    epochs=1000,\r\n",
    "    shuffle=True,\r\n",
    "    verbose=2\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Quantifying the Model\r\n",
    "\r\n",
    "# Evaluate the model using the testing data\r\n",
    "model_loss, model_accuracy = model.evaluate(\r\n",
    "    X_test_scaled, y_test_categorical, verbose=2)\r\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Making predictions with the new data\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "new_data = np.array([[0.2, 0.3, 0.4]])\r\n",
    "print(f\"Predicted class: {model.predict_classes(new_data)}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "401fa3fab014f3a08907cecc500de258789879d01d4727ddc61289adf1641c9f"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.10 64-bit ('pythondata': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}